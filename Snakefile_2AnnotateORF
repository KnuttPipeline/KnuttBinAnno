##
## 
##
##

# It is just a proxy for the Snakefile_KnuttReads2Bins. This is done to
# make the workflow callable without speciying the Snakefile location
# and also allow the "include"ing of this workflow without conflicts
#  on the "all" rule

metaerg_tmpl = config["output_dir"] + "/metaerg/{bin}"
gene_tmpl = config["output_dir"] + "/split/{bin}_cds." 
dbcan_tmpl = config["output_dir"] + "/dbCAN/{bin}/{bin}_dbCAN_"
hyddb_tmp = config["output_dir"] + "/HydDB/{bin}/{bin}_HydDB"
kofam_tmp = config["output_dir"] + "/KofamKOALA/{bin}/{bin}_KofamKOALA"
eggnog_tmp = config["output_dir"] + "/eggNOG/{bin}/{bin}_eggNOG"
iprscan_tmp = config["output_dir"] + "/InterProScan/{bin}/{bin}_iprscan"

concatwithheader = "awk '(NR == 1) || (FNR > 1)' {input} > {output}"

localrules: rrna, split_genes

rule metaergsamplecontigs:
	input:
		bin = bin_file_pattern,
		datadir = rules.downloadMetaErg.output,
		installdir = rules.installMetaErg.output
	output:
		cds = metaerg_tmpl + "/data/cds.faa",
		cdsn = metaerg_tmpl + "/data/cds.ffn",
		gff = metaerg_tmpl + "/data/master.gff.txt",
		rrna_tab = metaerg_tmpl + "/data/rRNA.tab.txt",
		mastertsv = metaerg_tmpl + "/data/master.tsv.txt"
	log:
		metaerg_tmpl + "/metaerg_run.log"
	params:
		locustag = "{bin}",
		dir = metaerg_tmpl
	conda:
		"envs/metaerg.yml"
	resources:
		mem_mb=4*1024,
		disk_mb=lambda wildcards,input:sum(i.size for i in input)+15*input.bin.size
	threads:
		8
	message: "Running metaerg on {input.bin}"
	shell:
		"{{ export MinPath=$(realpath {input.installdir})/MinPath ; export PATH=$PATH:$(realpath {input.installdir})/bin;  metaerg.pl -db {input.datadir} --mincontiglen {config[min_contiglen]} --minorflen {config[min_orflen]} --outdir {params.dir} --cpus {threads} {input.bin} --force --locustag {params.locustag} && samtools faidx {output.cds} ; }} &> {log}"

chunk_numbers = [str(i).zfill(len(str(config["chunk_count"]))) for i in range(config["chunk_count"])]

chunk = gene_tmpl + "{j}.faa"


# Combine these files
rule rrna:
	input:
		files = expand(metaerg_tmpl + "/data/rRNA.tab.txt", bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/rrna_tab_all.tsv"
	script:
		"scripts/dataConcat.py"


rule split_genes:
	input:
		rules.metaergsamplecontigs.output.cds
	params:
		newloc = f"{gene_tmpl}faa"
	output:
		temp(expand(chunk, j=chunk_numbers, bin="{bin}"))
	log:
		gene_tmpl = gene_tmpl + "log"
	conda:
		"envs/KnuttBinAnno.yml"
	message: "Splitting {input} into {config[chunk_count]} chunks."
	shell:
		"cp {input} {params.newloc} && pyfasta split -n {config[chunk_count]} {params.newloc} &> {log} ; rm {params.newloc}"


# Run diamond on the bins
dbcandiamonddbs_eval = {"CAZyDB":config["dbCAN_diamond_eval"],"tcdb":config["tcdb_diamond_eval"]}
rule bindbCANDIAMOND:
	input:
		db = rules.dbcan_diamond_index.output,
		query = chunk
	params:
		db = rules.dbcan_diamond_index.params.db,
		evalue = lambda w: dbcandiamonddbs_eval[w.db]
	output:
		dbcan_tmpl + "{j}_dmnd_{db}.tsv"
	log:
		dbcan_tmpl + "{j}_dmnd_{db}.log"
	conda:
		"envs/KnuttBinAnno.yml"
	threads:
		2
	shell:
		"diamond blastp --db {params.db} -k 1 --query {input.query} --out {output} --outfmt 6 --evalue {params.evalue} -p {threads} &> {log}"

# Run hmmscan on the bins
hmmdbs = {"cazy":rules.downloaddbCAN2hmmer.output.raw,"tf1":rules.downloadtfmodels.output.tf1,"tf2":rules.downloadtfmodels.output.tf2,"stp":rules.downloadtfmodels.output.stp}
rule hmmscan:
	input:
		db = lambda w: hmmdbs[w.hmm],
		query = chunk
	output:
		dbcan_tmpl + "{j}_hmmer_{hmm}.tsv"
	log:
		dbcan_tmpl + "{j}_hmmer_{hmm}.log"
	conda:
		"envs/oldhmmer.yml"
	threads:
		2
	shell:
		"hmmscan --domtblout {output} --cpu {threads} -o /dev/null {input.db} {input.query}"

hotpepfiles = ["add_functions_orf.py","parallel_group_many_proteins_many_patterns_noDNA.py","bact_group_many_proteins_many_patterns.py","list_multidomain_proteins.py","train_many_organisms_many_families.py"]
hotpepdirs = ["CAZY_PPR_patterns","fungus_fungus","Chaetomium_globosum_cbs_148_51"]
rule copyHotPEP:
	input:
		rules.downloaddbCAN2hotpep.output
	params:
		directory(config["output_dir"] + "/dbCAN_HotPep")
	output:
		expand(config["output_dir"] + "/dbCAN_HotPep/{file}", file=hotpepfiles),
		directory(expand(config["output_dir"] + "/dbCAN_HotPep/{dir}", dir=hotpepdirs))
	shell:
		"cp -R {input} {params}"

# Run HotPEP for a bin
rule hotpep:
	input:
		query = chunk,
		hotpep = rules.copyHotPEP.output
	params:
		dir=rules.copyHotPEP.output[0]+"/{j}_{bin}",
		reldir="{j}_{bin}",
		hits=config["hotpep_hits"], # 6
		freq=config["hotpep_freq"] #2.6
	output:
		dbcan_tmpl + "{j}_hotpep.tsv"
	log:
		dbcan_tmpl + "{j}_hotpep.log"
	conda:
		"envs/KnuttBinAnno.yml"
	threads:
		2
	shell:
		"{{ mkdir -p {params.dir} && python ./scripts/HotPEPFASTAsplit.py {input.query} {threads} {params.dir} && olddir=$(pwd) && cd {params.dir}/.. && python train_many_organisms_many_families.py {params.reldir} {threads} {config[hotpep_hits]} {config[hotpep_freq]} && cd $olddir && mv {params.dir}/Results/output.txt {output} && rm -r {params.dir} ; }} &> {log}"	

# Combine metadbCAN results
rule combinedbCAN:
	input:
		rules.copyHotPEP.output,
		diamond = expand(dbcan_tmpl + "{j}_dmnd_{db}.tsv",bin="{bin}",j="{j}",db="CAZyDB"),
		diamond_ref = rules.downloadCAZYdb.output.tsv,
		tcdb = expand(dbcan_tmpl + "{j}_dmnd_{db}.tsv",bin="{bin}",j="{j}",db="tcdb"),
		tcdb_ref = rules.downloadtcdb.output.seqs,
		hmmscan = expand(dbcan_tmpl + "{j}_hmmer_{hmm}.tsv",bin="{bin}",j="{j}",hmm="cazy"),
		tf1hmmscan = expand(dbcan_tmpl + "{j}_hmmer_{hmm}.tsv",bin="{bin}",j="{j}",hmm="tf1"),
		tf2hmmscan = expand(dbcan_tmpl + "{j}_hmmer_{hmm}.tsv",bin="{bin}",j="{j}",hmm="tf2"),
		stphmmscan = expand(dbcan_tmpl + "{j}_hmmer_{hmm}.tsv",bin="{bin}",j="{j}",hmm="stp"),
		hotpep = rules.hotpep.output,
		query = chunk
	params:
		hotpepref = rules.copyHotPEP.params[0],
		hmmscaneval=config["cazy_hmmscan_eval"],
		hmmscancov=config["cazy_hmmscan_cov"],
		stp_hmmscaneval=config["stp_hmmscan_eval"],
		stp_hmmscancov=config["stp_hmmscan_cov"],
		tf_hmmscaneval=config["tf_hmmscan_eval"],
		tf_hmmscancov=config["tf_hmmscan_cov"],
	output:
		dbcan_tmpl + "{j}_all.tsv"
	conda:
		"envs/R.yml"
	script:
		"scripts/dbCAN.R"
		
# Combine the chunk files
rule dbcanchunkconcat:
	input:
		expand(dbcan_tmpl + "{j}_all.tsv",bin="{bin}", j=chunk_numbers)
	output:
		dbcan_tmpl + "all.tsv"
	shell:
		concatwithheader
    

# Combine these files
rule dbCAN:
	input:
		files = expand(dbcan_tmpl + "all.tsv", bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/dbCAN_all.tsv"
	script:
		"scripts/dataConcat.py"

# Run cgc finder reimplement
rule cgcfinder:
	input:
		dbcan = rules.dbcanchunkconcat.output,
		query = rules.split_genes.input
	params:
		spacersig=config["dbcan_cgc_maxnotdbcangenespacers"] # 1
	output:
		dbcan_tmpl + "cgc.tsv"
	conda:
		"envs/R.yml"
	script:
		"scripts/dbCAN-CGCFinder.R"

# Combine these files
rule cgc:
	input:
		files = expand(rules.cgcfinder.output, bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/dbCAN_all.tsv"
	script:
		"scripts/dataConcat.py"

# Analyze hydrogenases
rule hyddbsample:
	input:
		rules.hyddbrps.output,
		cds = rules.split_genes.input,
	params:
		dbdir = rules.hyddbrps.params.dir,
		eval = 10E-10
	output:
		hyddb_tmp + ".tsv"
	log:
		hyddb_tmp + ".log"
	conda:
		"envs/hyddbclient.yml"
	shell:
		"python3 scripts/hyddbclient/hyddbsubmit.py -o {output} -l {log} -f {params.dbdir} -e {params.eval} {input.cds}"

# Combine these files
rule hyddb:
	input:
		files = expand(hyddb_tmp + ".tsv", bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/hyddb_all.tsv"
	script:
		"scripts/dataConcat.py"

# Execute kofamkoala
rule kofamkoalabin:
	input:
		kolist = rules.downloadkofam.output.kolist,
		profiles = rules.downloadkofam.output.profiles,
		query = chunk
	output:
		kofam_tmp + "_{j}.tsv"
	log:
		kofam_tmp + "_{j}.log"
	conda:
		"envs/KnuttBinAnno.yml"
	threads:
		2
	shell:
		"tmpd=$(mktemp -d -p .)  && exec_annotation -o {output} -p {input.profiles} -k {input.kolist} --cpu={threads} {input.query} --tmp-dir $tmpd &> {log} && rm -r $tmpd"


# Combine chunk data
rule kofamkoalachunkconcat:
	input:
		expand(kofam_tmp + "_{j}.tsv", bin="{bin}", j=chunk_numbers)
	output:
		kofam_tmp + ".tsv"
	shell:
		concatwithheader


# Create prot tsv and ko map from kofam
rule kofamgff:
	input:
		kofam = rules.kofamkoalachunkconcat.output,
		query = rules.split_genes.input
	params:
		ifunsure_eval = config["kofam_helper_eval"],
	output:
		gffbase = kofam_tmp + "_gffbase.tsv",
		kos = kofam_tmp + "_kos_all.tsv",
		kos_onlysure = kofam_tmp + "_kos_filtered.tsv"
	conda:
		"envs/R.yml"
	script:
		"scripts/kofam2gff.R"

# Find modules
rule kofammodules:
	input:
		kofam_tmp + "_kos_{type}.tsv",
	output:
		kofam_tmp + "_kos_{type}_modules.tsv"
	wildcard_constraints:
		type = "all|filtered"
	conda:
		"envs/R.yml"
	script:
		"scripts/KOAnalyze.R"
rule kofam:
	input:
		expand(kofam_tmp + "_kos_{type}_modules.tsv",type=("all","filtered"),bin=bins)


# Run interproscan on one bin
rule interproscanbin:
	input:
		query=rules.split_genes.input,
		install=rules.downloadInterProScan.output
	params:
		outbase = iprscan_tmp + "_{j}"
	output:
		expand(iprscan_tmp + "_{j}.{ext}",ext=["tsv","xml","gff3"],allow_missing=True)
	log:
		iprscan_tmp + "_{j}.log"
	conda:
		"envs/interproscan.yml"
	threads:
		4
	shell:
		"{input.install}/interproscan.sh --cpu {threads} -ms {config[min_orflen]} -b {params.outbase} -T $(mktemp -d) -f TSV,XML,GFF3 -goterms -i {input.query} -pa -t p &> {log}"


# Combine chunk data
rule interprochunkconcat:
	input:
		expand(iprscan_tmp + "_{j}.tsv", bin="{bin}", j=chunk_numbers)
	output:
		iprscan_tmp + ".tsv"
	shell:
		concatwithheader

# Combine these files
rule interpro:
	input:
		files = expand(iprscan_tmp + ".tsv", bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/interproscan_all.tsv"
	script:
		"scripts/dataConcat.py"

# Run eggnog-mapper on one bin
rule eggnogmapbin:
	input:
		rules.downloadeggnogdata.output,
		query = rules.split_genes.input
	params:
		base =  eggnog_tmp + "_{j}",
		dir = rules.downloadeggnogdata.params.dir
	output:
		seedorthologs = eggnog_tmp + "_{j}.emapper.seed_orthologs",
		annotations = eggnog_tmp + "_{j}.emapper.annotations"
	log:
		eggnog_tmp + "_{j}.emapper.log"
	conda:
		"envs/metaerg.yml"
	threads:
		4
	shell:
		"{params.dir}/eggnog-mapper/emapper.py --seed_ortholog_evalue {config[eggnog_eval]} --seed_ortholog_score {config[eggnog_minscore]} --target_orthologs all --data_dir {params.dir}/data -m diamond -i {input.query} --cpu {threads} -o {params.base} &> {log}"



# Create prot gff from eggnog
rule eggnoggff:
	input:
		anno=rules.eggnogmapbin.output.annotations,
		query=rules.hotpep.input.query
	output:
		eggnog_tmp + "_{j}.emapper.tsv"
	conda:
		"envs/R.yml"
	script:
		"scripts/eggnog2gff.R"



# Combine chunk data
rule eggnogchunkconcat:
	input:
		expand(eggnog_tmp + "_{j}.emapper.tsv", bin="{bin}", j=chunk_numbers)
	output:
		eggnog_tmp + ".tsv"
	shell:
		concatwithheader

# Combine these files
rule eggnog:
	input:
		files = expand(eggnog_tmp + ".tsv", bin=bins)
	params:
		colnames = ["bin"],
		vals = bins
	output:
		out = config["output_dir"] + "/eggnog_all.tsv"
	script:
		"scripts/dataConcat.py"