##
## 
##
##

# It is just a proxy for the Snakefile_KnuttReads2Bins. This is done to
# make the workflow callable without speciying the Snakefile location
# and also allow the "include"ing of this workflow without conflicts
#  on the "all" rule

basedir_dbs = config["reference_dir"]


# repr([str(p) for p in Path('.').rglob('*') if p.is_file()])

metaergdir = basedir_dbs+"/metaerg_install"
metaergdatadir = basedir_dbs+"/metaerg_db"


# Install MetaErg
rule installMetaErg:
	output:
		directory(metaergdir)
	log:
		basedir_dbs+"/install_metaerg.log"
	conda:
		"envs/metaerg.yml"
	resources:
		mem_mb=2048,
		disk_mb=3000
	message: "Installing metaerg."
	shell:
		("{{ bash scripts/BuildDBs/installMetaErg.sh {output} && "
		"patch {output}/metaerg/bin/output_reports.pl  scripts/BuildDBs/metaergPathways.patch ;  }} &> {log} ")

# Download MetaErg data
rule downloadMetaErg:
	output:
		directory(metaergdatadir)
	conda:
		"envs/KnuttBinAnno.yml"
	resources:
		disk_mb=50000,
		mem_mb=2048
	message: "Downloading metaerg data"
	shell:
		"mkdir -p {output} && wget http://ebg.ucalgary.ca/metaerg/db.tar.gz -qO- | tar xzf - -C {output} --strip-components=1"


##
## dbCAN
##

dbcan_fasta_files = basedir_dbs + "/dbCAN2/seqs/"
dbcan_data_files = basedir_dbs + "/dbCAN2/data/"
dbcan_hmmer_files = basedir_dbs + "/dbCAN2/hmm/"

# Download the CAZYdb from dbCAN2
rule downloadCAZYdb:
	output:
		fasta = dbcan_fasta_files + "CAZyDB.fasta",
		tsv = dbcan_data_files + "CAZyDB.tsv",
		descr = dbcan_data_files + "CAZyDB-fam-activities.txt"
	conda:
		"envs/KnuttBinAnno.yml"
	shell:
		("echo 'sseqid\tCAZyECs' > {output.tsv} && "
		"wget -qO- http://bcb.unl.edu/dbCAN2/download/Databases/CAZyDB.07312019.fa | tr '|' ' ' | "
		"tee {output.fasta} | grep '>' | tr -d '>' | sed 's/ /\t/' >> {output.tsv} && "
		"wget -qO {output.descr} http://bcb.unl.edu/dbCAN2/download/Databases/CAZyDB.07312018.fam-activities.txt")


# Download dbCAN2 hmmer models
rule downloaddbCAN2hmmer:
	output:
		raw = dbcan_hmmer_files + "dbCAN-HMMdb.txt",
		index = expand(dbcan_hmmer_files + "dbCAN-HMMdb.txt.{ext}",ext=["h3i","h3m","h3f","h3p"])
	conda:
		"envs/oldhmmer.yml"
	shell:
		"wget -qO {output.raw} http://bcb.unl.edu/dbCAN2/download/dbCAN-HMMdb-V8.txt && hmmpress {output.raw}"

# Download tf,stp models
rule downloadtfmodels:
	output:
		tf1 = dbcan_hmmer_files + "tf-1.hmm",
		tf2 = dbcan_hmmer_files + "tf-2.hmm",
		stp = dbcan_hmmer_files + "stp.hmm",
		indices = expand(dbcan_hmmer_files + "{model}.hmm.{ext}",model=["tf-1","tf-2","stp"],ext=["h3i","h3m","h3f","h3p"])
	conda:
		"envs/oldhmmer.yml"
	shell:
		("wget -qO {output.tf1} http://bcb.unl.edu/dbCAN2/download/Databases/tf-1.hmm && hmmpress {output.tf1} && "
		"wget -qO {output.tf2} http://bcb.unl.edu/dbCAN2/download/Databases/tf-2.hmm && hmmpress {output.tf2} && "
		"wget -qO {output.stp} http://bcb.unl.edu/dbCAN2/download/Databases/stp.hmm && hmmpress {output.stp}")

# Download tcdb files
rule downloadtcdb:
	output:
		seqs = dbcan_fasta_files + "tcdb.fasta",
	conda:
		"envs/KnuttBinAnno.yml"
	shell:
		"wget -qO {output.seqs} http://bcb.unl.edu/dbCAN2/download/Databases/tcdb.fa"
		

# Download dbCANs HotPep
rule downloaddbCAN2hotpep:
	output:
		directory(basedir_dbs + "/dbCAN2/HotPEP")
	conda:
		"envs/KnuttBinAnno.yml"
	shell:
		"mkdir {output} && wget -qO- http://bcb.unl.edu/dbCAN2/download/Tools/hotpep-python-08-20-2019.tar.gz | tar -C {output} -xzf -"

rule dbcan_diamond_index:
	input:
		seqs = dbcan_fasta_files + "{db}.fasta"
	params:
		db = dbcan_fasta_files + "{db}"
	output:
		dbcan_fasta_files + "{db}.dmnd"
	log:
		dbcan_fasta_files + "{db}.dmnd.log"
	threads: 30
	conda:
		"envs/KnuttBinAnno.yml"
	shell:
		"diamond makedb -p {threads} --db {params.db} --in {input.seqs} &> {log}"

# Create RPS blast db for hyddb filtering
rule hyddbrps:
	params:
		dir=basedir_dbs+"/hyddbclient"
	output:
		expand(basedir_dbs+"/hyddbclient/hyd.{ext}",ext=["aux","freq","pin","loo","rps"])
	log:
		basedir_dbs+"/hyddbclient.log"
	conda:
		"envs/hyddbclient.yml"
	shell:
		"python3 scripts/hyddbclient/createHydRPSdb.py {params.dir} &> {log}"

# Download InterProScan
interproscanversion = "5.40-77.0"	
pantherversion="14.1"
rule downloadInterProScan:
	output:
		directory(basedir_dbs+"/interproscan-"+interproscanversion)
	params:
		url = "ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/"+interproscanversion+"/interproscan-"+interproscanversion+"-64-bit.tar.gz",
		targetdir = basedir_dbs,
		pantherurl="ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/5/data/panther-data-"+pantherversion+".tar.gz"
	shell:
		"wget -qO- {params.url} | tar -C {params.targetdir} -xzf - && wget -qO- {params.pantherurl} | tar -C {output}/data -pxzf -"

# Download kofamkoala and kofamscan
rule downloadkofam:
	params:
		dir = basedir_dbs + "/kofam/"
	output:
		kolist = basedir_dbs + "/kofam/ko_list",
		profiles = directory(basedir_dbs + "/kofam/profiles")
	conda:
		"envs/KnuttBinAnno.yml"
	shell:
		("wget -qO- ftp://ftp.genome.jp/pub/db/kofam/ko_list.gz | gunzip > {output.kolist} && "
		"wget -qO- ftp://ftp.genome.jp/pub/db/kofam/profiles.tar.gz | tar -xzf - -C {params.dir}" )

# Download eggnogmapper data
rule downloadeggnogdata:
	output:
		expand(basedir_dbs+"/eggnog/data/{file}",file=["eggnog.db","eggnog_proteins.dmnd"]),
		directory(basedir_dbs+"/eggnog/eggnog-mapper"),
	log:
		basedir_dbs+"/eggnog.log"
	params:
		dir = basedir_dbs+"/eggnog"
	conda:
		"envs/metaerg.yml"
	shell:
		" {{ git clone --branch 2.0.1 https://github.com/eggnogdb/eggnog-mapper.git {params.dir}/eggnog-mapper && {params.dir}/eggnog-mapper/download_eggnog_data.py -f -y --data_dir {params.dir}/data ; }} &> {log}"
